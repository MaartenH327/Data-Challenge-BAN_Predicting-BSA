{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fie0rkE3-_4"
      },
      "outputs": [],
      "source": [
        "#import pandas etc\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGLLJ83R4BtO",
        "outputId": "643b620e-7302-4243-ae08-585e78a2f15c"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "file_path = '...'\n",
        "sheet_name = 3  # Specify the sheet name here\n",
        "#0 for program 1, 1 for program 2 etc\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_course_columns(df):\n",
        "    course_columns = [col for col in df.columns if col.startswith('Course')]\n",
        "    first_try_columns = [col for col in course_columns if col.endswith('-1')]\n",
        "    resit_columns = [col for col in course_columns if col.endswith('-R')]\n",
        "    final_grade_columns = [col for col in course_columns if not (col.endswith('-1') or col.endswith('-R'))]\n",
        "\n",
        "    return first_try_columns, resit_columns, final_grade_columns\n",
        "\n",
        "# Extract course columns\n",
        "first_try_columns, resit_columns, final_grade_columns = extract_course_columns(df)\n",
        "\n",
        "print(\"First Try Columns:\", first_try_columns)\n",
        "print(\"Resit Columns:\", resit_columns)\n",
        "print(\"Final Grade Columns:\", final_grade_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SewMjek-4Qye",
        "outputId": "983ec10a-d441-4ba5-c948-b308c2cd6653"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store column names\n",
        "block1_results = []\n",
        "\n",
        "# Iterate through each column in first_try_columns\n",
        "for col in first_try_columns:\n",
        "    # Extract the course number X from the column name\n",
        "    course_num = col.split('-')[0]\n",
        "\n",
        "    # Check if there's a corresponding resit column (CourseX-R)\n",
        "    resit_col = f\"{course_num}-R\"\n",
        "    if resit_col in resit_columns:\n",
        "        # Add the initial attempt column (CourseX-1) to the list\n",
        "        block1_results.append(col)\n",
        "\n",
        "# Print the resulting list of column names\n",
        "print(\"block1_results:\", block1_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FmY4aWb4Bpx"
      },
      "outputs": [],
      "source": [
        "# Create resits_needed column and initialize with zeros\n",
        "df['resits_needed'] = 0\n",
        "\n",
        "# Loop through each column in first_try_columns\n",
        "for col in block1_results:\n",
        "    # Increment resits_needed by 1 if the column value is less than 5.5 or missing\n",
        "    df.loc[(df[col] < 5.5) | (df[col].isna()), 'resits_needed'] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM1fsxEa4BkF",
        "outputId": "18d62f35-bc70-4cda-e7b0-d226e5766eac"
      },
      "outputs": [],
      "source": [
        "# List of column names\n",
        "column_names = block1_results\n",
        "\n",
        "# Dictionary to store means\n",
        "means = {}\n",
        "\n",
        "# Calculate mean for each column\n",
        "for column in column_names:\n",
        "    means[column] = df[column].mean()\n",
        "\n",
        "# Calculate mean_program1B1B2\n",
        "mean_programB1 = sum(means.values()) / len(means)\n",
        "mean_programB1 = round(mean_programB1, 2)\n",
        "\n",
        "# Calculate average_gradeB1B2\n",
        "df['average_gradeB1'] = df[block1_results].mean(axis=1)\n",
        "df['average_gradeB1'] = round(df['average_gradeB1'], 2)\n",
        "\n",
        "df['deviationB1'] = df['average_gradeB1'] - mean_programB1\n",
        "df['deviationB1'] = round(df['deviationB1'], 2)\n",
        "\n",
        "\n",
        "# Print means\n",
        "for column, mean in means.items():\n",
        "    print(f\"Mean for {column}: {round(mean, 2)}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Mean for Program B1:\", mean_programB1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSIxgqV54Bhd"
      },
      "outputs": [],
      "source": [
        "df['deviationB1'] = df['average_gradeB1'] - mean_programB1\n",
        "df['deviationB1'] = round(df['deviationB1'], 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch_nDItc7rKh"
      },
      "outputs": [],
      "source": [
        "# Initialize the 'absent' column with zeros\n",
        "df['absent'] = 0\n",
        "\n",
        "# Iterate through each column in block1and2_results\n",
        "for col in block1_results:\n",
        "    # Extract the course number X from the column name\n",
        "    course_num = col.split('-')[0]\n",
        "\n",
        "    # Check if it's an initial attempt column\n",
        "    if col.endswith('-1'):\n",
        "        # Increment 'absent' by 1 if the initial attempt column is missing\n",
        "        df.loc[df[col].isna(), 'absent'] += 1\n",
        "    elif col.endswith('-R'):\n",
        "        # Check if there's a corresponding initial attempt column and increment 'absent' accordingly\n",
        "        initial_col = f\"{course_num}-1\"\n",
        "        df.loc[(df[col].isna()) & ((df[initial_col] < 5.5) | df[initial_col].isna()), 'absent'] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbXinfij9krQ",
        "outputId": "fb4ac659-f557-4de7-8383-4680ed7b4696"
      },
      "outputs": [],
      "source": [
        "column_names = block1_results\n",
        "means = {}\n",
        "for column in column_names:\n",
        "    means[column] = df[column].mean()\n",
        "\n",
        "mean_programB1 = sum(means.values())/ len(means)\n",
        "mean_programB1 = round(mean_programB1, 2)\n",
        "\n",
        "df['average_gradeB1'] = df[block1_results].mean(axis=1)\n",
        "df['average_gradeB1'] = round(df['average_gradeB1'], 2)\n",
        "\n",
        "df['deviationB1'] = df['average_gradeB1'] - mean_programB1\n",
        "df['deviationB1'] = round(df['deviationB1'], 2)\n",
        "\n",
        "for column, mean in means.items():\n",
        "    print(f'Mean for {column}: {round(mean, 2)}')\n",
        "\n",
        "print('Mean for Program B1:', mean_programB1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnwRs25k9kh-"
      },
      "outputs": [],
      "source": [
        "# Create 'Dutch' column with True/False values based on the condition\n",
        "df['Dutch'] = (df['Nationality'] == 'Nederland')\n",
        "\n",
        "\n",
        "df['Dutch'] = df['Dutch'].astype(int)\n",
        "\n",
        "# Create 'Dutch' column with True/False values based on the condition\n",
        "df['Non-Dutch'] = (df['Nationality'] != 'Nederland')\n",
        "\n",
        "\n",
        "df['Non-Dutch'] = df['Non-Dutch'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmdcXpgp9ken"
      },
      "outputs": [],
      "source": [
        "# Iterate through block1and2_results to create passed columns\n",
        "for col in block1_results:\n",
        "    # Extract the course number and attempt type from the column name\n",
        "    course_num, attempt_type = col.split('-')\n",
        "\n",
        "    # Check if it's an initial attempt column\n",
        "    if attempt_type == '1':\n",
        "        # Create the passed column if it doesn't exist\n",
        "        if f'passed{course_num}' not in df.columns:\n",
        "            df[f'passed{course_num}'] = 0\n",
        "\n",
        "        # Set passed column to 1 if the grade is greater than or equal to 5.5\n",
        "        df.loc[df[col] >= 5.5, f'passed{course_num}'] = 1\n",
        "\n",
        "    # Check if it's a resit attempt column\n",
        "    elif attempt_type == 'R':\n",
        "        # Set passed column to 1 if the resit attempt is greater than or equal to 5.5\n",
        "        df.loc[df[col] >= 5.5, f'passed{course_num}'] = 1\n",
        "\n",
        "        # Set passed column to 1 if the resit attempt is missing and the initial attempt is greater than or equal to 5.5\n",
        "        initial_col = next((c for c in block1_results if c.startswith(f'Course{course_num}-1')), None)\n",
        "        if initial_col:\n",
        "            df.loc[(df[col].isna()) & (df[initial_col] >= 5.5), f'passed{course_num}'] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA-IgdBE-TlX"
      },
      "outputs": [],
      "source": [
        "# Initialize the YEAR column with zeros\n",
        "df['YEAR'] = 0\n",
        "\n",
        "# Loop through each column specified in the final_grade_columns list\n",
        "for var in final_grade_columns:\n",
        "    df['YEAR'] += df[var].apply(lambda x: 6 if x >= 5.5 and not pd.isnull(x) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the YEAR column with zeros\n",
        "df['Credits-B1'] = 0\n",
        "\n",
        "# Loop through each column specified in the final_grade_columns list\n",
        "for var in block1_results:\n",
        "    df['Credits-B1'] += df[var].apply(lambda x: 6 if x >= 5.5 and not pd.isnull(x) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW6bQS7Y-hA1"
      },
      "outputs": [],
      "source": [
        "# Generate the passed42 column and convert boolean to integer\n",
        "df['passed42'] = (df['YEAR'] >= 42).astype(int)\n",
        "df['passed36'] = (df['YEAR'] >= 36).astype(int)\n",
        "df['passed48'] = (df['YEAR'] >= 48).astype(int)\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Gender', 'PreEducation'], dtype=int)\n",
        "\n",
        "\n",
        "non_categorical_columns = [col for col in df.columns if col not in ['Gender', 'Nationality', 'PreEducation', 'Program', 'Year', 'BSA']]\n",
        "df[non_categorical_columns] = df[non_categorical_columns].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILvKByfW-g93"
      },
      "outputs": [],
      "source": [
        "#drop columns outside of block 1\n",
        "columns_to_drop = ['Credits-Y1', 'BSA','Program', 'YEAR','Year', 'Nationality', 'Crd-B1B2']\n",
        "\n",
        "\n",
        "\n",
        "# Drop the specified columns\n",
        "df.drop(columns=columns_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCwt3kZ2-g6w"
      },
      "outputs": [],
      "source": [
        "# Drop the columns specified in final_grade_columns\n",
        "df.drop(columns=final_grade_columns, inplace=True)\n",
        "\n",
        "df.drop(columns= resit_columns, inplace=True)\n",
        "\n",
        "columnstodrop = [col for col in first_try_columns if col not in block1_results]\n",
        "df.drop(columns=columnstodrop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPRygbCQ-g3n"
      },
      "outputs": [],
      "source": [
        "#load train and test data\n",
        "train_file_path = '...'\n",
        "test_file_path = '...'\n",
        "\n",
        "train_df = pd.read_excel(train_file_path, sheet_name=sheet_name)\n",
        "test_df = pd.read_excel(test_file_path, sheet_name=sheet_name)\n",
        "\n",
        "\n",
        "train_indices = df[\"train\"].isin(train_df[\"train\"])\n",
        "test_indices = df[\"train\"].isin(test_df[\"train\"])\n",
        "\n",
        "X = df.drop(columns = ['passed36','passed42','passed48'])\n",
        "y = df['passed42']\n",
        "\n",
        "X_train = X.loc[train_indices].drop(columns=[\"train\"])\n",
        "y_train = y.loc[train_indices]\n",
        "\n",
        "X_test = X.loc[test_indices].drop(columns=[\"train\"])\n",
        "y_test = y.loc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLTwf6Hy-4ye"
      },
      "outputs": [],
      "source": [
        "# Create a baseline Random Forest Classifier\n",
        "baseline_rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the baseline model\n",
        "baseline_rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_baseline = baseline_rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the baseline model\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "precision_baseline = precision_score(y_test, y_pred_baseline, average='binary')\n",
        "recall_baseline = recall_score(y_test, y_pred_baseline, average='binary')\n",
        "f1_baseline = f1_score(y_test, y_pred_baseline, average='binary')\n",
        "\n",
        "# Print the baseline metrics\n",
        "print('Baseline Model Performance:')\n",
        "print(f'Accuracy: {accuracy_baseline}')\n",
        "print(f'Precision: {precision_baseline}')\n",
        "print(f'Recall: {recall_baseline}')\n",
        "print(f'F1 Score: {f1_baseline}')\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_baseline))\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred_baseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfqO4kuZ-5wp"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by Grid Search\n",
        "print(f'Best parameters found: {grid_search.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH9RHfQf-5tr"
      },
      "outputs": [],
      "source": [
        "# Create the model with the best parameters found by Grid Search\n",
        "best_rf_classifier = RandomForestClassifier(\n",
        "    max_depth=grid_search.best_params_['max_depth'],\n",
        "    max_features=grid_search.best_params_['max_features'],\n",
        "    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
        "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
        "    n_estimators=grid_search.best_params_['n_estimators'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_best = best_rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model with the best parameters\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "precision_best = precision_score(y_test, y_pred_best, average='binary')\n",
        "recall_best = recall_score(y_test, y_pred_best, average='binary')\n",
        "f1_best = f1_score(y_test, y_pred_best, average='binary')\n",
        "\n",
        "# Print the metrics for the best model\n",
        "print('Best Model Performance:')\n",
        "print(f'Accuracy: {accuracy_best}')\n",
        "print(f'Precision: {precision_best}')\n",
        "print(f'Recall: {recall_best}')\n",
        "print(f'F1 Score: {f1_best}')\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_best))\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred_best))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbPakPRm-5oG"
      },
      "outputs": [],
      "source": [
        "# Print the predictions made by the best model\n",
        "print(\"Best Model Predictions:\")\n",
        "print(y_pred_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L9u78NH-5k-"
      },
      "outputs": [],
      "source": [
        "# Get the feature importances of the best model\n",
        "feature_importances_best = best_rf_classifier.feature_importances_\n",
        "\n",
        "\n",
        "feature_names = X_test.columns\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df_best = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances_best\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "importance_df_best = importance_df_best.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print('Feature Importances of the Best Model:')\n",
        "print(importance_df_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6_fF4Vs-5hz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df_best)\n",
        "plt.title('Feature Importances of the Best Model')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTutGgyN_Jrw"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = X_train.corr()\n",
        "\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmy7m7Sw_Jon"
      },
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL2dZQ5__Jlz"
      },
      "outputs": [],
      "source": [
        "total_score = accuracy_best + recall_best + precision_best + f1_best\n",
        "print(\"Total score\", round(total_score, 2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
